# 모델 양자화 (Model Quantization) 누구나 쉽게 이해하기

양자화라는 단어를 처음 들으면, 뭔가 양자 컴퓨터가 떠오를 수 있습니다. 아니면 복잡하고 이해하기 어려운 개념이나 수식이 연상될지도 모릅니다. 처음에 저 역시 양자화를 접했을 때 그랬었거든요.

그러나 다행인 점은 양자 컴퓨팅과 양자화는 관련이 없으며, 더 다행인점은 양자화는 상당히 간단한 개념이라는 것입니다.

## 구구단

-- 구구단을 외자 -- 이미지 넣기

양자화를 설명하기 전에 간단한 곱셈을 해보겠습니다. 7 * 8은 얼마일까요? 대부분 바로 56이라고 대답할 수 있을 겁니다. 이제 조금 복잡한 곱셈을 해볼까요? 7.0129 * 8.0236은 얼마일까요? 이 계산도 그렇게 빠르게 할 수 있을까요? 대부분 계산기를 사용할 것입니다. 계산 결과 56.2687이 나왔습니다. 값을 반올림하면 56이네요.

왜 7 * 8은 쉽고 7.0129 * 8.0236은 어려운 걸까요? 정수를 곱하는 것은 소수점이 있는 숫자를 곱하는 것보다 훨씬 쉽기 때문입니다. 그렇다면 빠른 계산을 위해서는 대충 계산하면 어떨까요? 7.0129와 8.0236을 각각 7과 8로 반올림하고, 이들을 곱해 56이라는 대략적인 답을 얻는 것이죠. 56.2687과 56은 거의 차이가 나지 않습니다. 이것이 바로 양자화의 기본 개념입니다.

## float32에서 float16으로 내려가다.

양자화에 대해 설명하기 전에, float32와 float16의 개념을 알아보겠습니다. 대부분의 모델은 특수한 경우를 제외하고 수많은 float32(대략 0.012901306과 같은 값) 행렬을 가지고 있습니다. 모델에 입력을 넣을 때도 float32 형식을 사용합니다.

모델의 가중치 행렬과 입력 행렬이 왜 float32일까요? 이유는 간단합니다. 대부분의 GPU 코어들은 float32 연산에 최적화되어 있기 때문입니다.

하지만 모델이 점점 커지고 연산량이 많아지면서, 속도가 느려지고 GPU 메모리가 부족해지는 문제가 발생했습니다. 이 문제를 해결하기 위해 NVIDIA는 2016년에 Tesla P100이라는 데이터센터 GPU를 출시했습니다.

-- P100 GPU 이미지 넣기 --

P100 GPU부터는 float16 연산을 공식적으로 지원하기 시작했습니다. 많은 딥러닝 연구원들은 이를 활용하여 연구를 진행했고, 2017년에는 Mixed Precision Training이라는 논문이 발표되어 float16이 딥러닝에도 유효하다는 것을 입증했습니다. 이 덕분에 학습 시 GPU 메모리 사용량을 절반으로 줄이면서도 더 큰 모델을 학습할 수 있게 되었습니다.

float16의 성공에 힘입어, V100 같은 후속 아키텍처에서는 float16과 int8 연산을 위한 전용 칩인 Tensor Cores가 추가되었습니다. 최근에는 H100이 출시되면서 float8까지 지원하고 있습니다.

## float16으로 모델 양자화

모델 학습이 아닌 모델 추론에 관심이 있는 연구원들도 이 좋은 float16을 사용하고자 하였습니다. 추론에 적용하는 것은 학습하는 것 보다 훨씬 더 간단했습니다. float32의 가중치를 그냥 반올림해서 float16으로 바꾸고 입력도 float16으로 바꾼 것입니다. 즉 모델에 대해 단순하게 양자화를 시킨 것입니다. 막상 이렇게 양자화를 하고 값을 뽑아내니 신기하게도 정확도 손실이 거의 없었습니다.

이유는 딥러닝 모델이 긴 체인 형태로 구성되어 있어서, float32에서 float16으로 내려가도 모델의 결론 도출에 큰 지장이 없다라는게 중론입니다. 그래서 현재는 모델 추론은 float16은 일단 기본으로 두고 최적화를 진행을 하고 있습니다.

사족 - 많은 사람들이 float16 네트워크는 너무나도 당연하게 잘 된다고 생각하고 있어 최근까지 위의 내용에 대한 딱히 실험 연구가 없었는데, 이를 잘 정리하여 낸 논문을 글 작성 중에 발견했습니다. 제목은 [Comparative Study: Standalone IEEE 16-bit Floating-Point for Image Classification](!https://arxiv.org/pdf/2305.10947.pdf)이며 결론을 보면 역시나 float16 네트워크가 float32와 Mixed Precision에 동등한 성능을 낼 수 있다고 합니다.

## float32에서 int8로 더 내려가다.

욕심 많은 연구원들은 여기에 만족하지 않고, float16보다 더 가벼운 int8을 사용하려 했습니다. 연구 끝에, 학습에서는 int8을 사용하는 것이 어렵고, 추론에서 int8 양자화를 적용할 수 있다는 결론을 내렸습니다. float32에서 float16으로 내려갈 때처럼 간단한 반올림이 아니라, 양자화 과정에 약간의 기술이 필요합니다.

## PTQ(Post Training Quantization)



